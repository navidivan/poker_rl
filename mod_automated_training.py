# -*- coding: utf-8 -*-
"""Untitled50.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b_EybrZ2zpfdTo0FY11D6ZPDaLwLw2Mo
"""
import numpy as np
import copy
import pandas as pd
import torch as T

from mod_poker_5 import *
from mod_fe import *
from mod_agents import *
from mod_memory import *
from mod_DQN_Conv import *
from mod_step_func import *
from mod_comp_test import *



from IPython import display
import matplotlib.pyplot as plt
import time
import copy
############################################################################

def visualize_lr_find (agent_a, agent_b, n_buffers=2500, lower_lr = 1e-6, upper_lr = 1):
  load_checkpoint = False
  n_games = 1
  n_steps = 0
  eps_history = []
  scores = []
  agent_a.memory.Make_Memory(a= agent_a, b=agent_b)
  fig,ax1 = plt.subplots()
  ax2 = ax1.twinx()
  iters = n_buffers
  ratio = upper_lr/lower_lr
  mult = ratio ** (1/iters)
  lrs=[]
  losses=[]

  for i in range(n_games):
    for j in range(n_buffers):
      agent_a.learn()
      lr = lower_lr * (mult ** j)
      lrs.append(lr)
      agent_a.q_eval_1.optimizer.param_groups[0]['lr'] = lr
      n_steps +=1
      eps_history.append(agent_a.epsilon)

    if i%2 ==0:
      agent_a.epsilon=0
      scores.append(bench(agent_a, num_rounds=2))
      print('generating memory')
      agent_a.epsilon=eps_history[-1]
      ser = pd.Series(data=agent_a.hist)
      ax1.plot(ser.rolling(10).mean(), color ='b')
      ax2.plot(eps_history, color = 'y')
      display.clear_output(wait=True)
      display.display(plt.gcf())
      print(scores)
      print(scores[-1])
      print(agent_a.epsilon, 'epsilon')
      print(i, "game number")

  fig, ax = plt.subplots()
  plt.plot(lrs,pd.Series(agent_a.hist).rolling(500).mean())
  ax.set_xscale('log')
############################################################################


import copy
def find_max_lr (agent_a, lower_lr=1e-6, upper_lr=0.01, epochs = 1):
  a_backup = copy.deepcopy(agent_a)
  a_backup.hist =[]
  n_buffers = epochs * int(a_backup.memory.obs_now.shape[0]/a_backup.batch_size)
  iters = n_buffers
  ratio = upper_lr/lower_lr
  mult = ratio ** (1/iters)
  lrs=[]
  initial_losses = []
  total_steps = n_buffers/10
  step= 0
  print(n_buffers, 'max_lr search n_buffers')
  print(a_backup.memory.obs_now.shape[0], 'memory size')
  for j in range(n_buffers):
    a_backup.learn()
    if step <=total_steps:
      step+=1
      initial_losses.append(a_backup.hist[-1])
    lr = lower_lr * (mult ** j)
    lrs.append(lr)
    a_backup.q_eval_1.optimizer.param_groups[0]['lr'] = lr
  serb = pd.Series(data=a_backup.hist).rolling(100).mean()
  index = np.argmin(serb)
  improve = np.min(serb)/ np.mean(initial_losses)
  max_lr = lrs[index]/4
  # initial_mean_loss = serb[0:10].mean()
  return max_lr, lrs, serb, improve
############################################################################

def cycle_lr_learn(agent_a, max_lr, epochs=1):
  a_backup = copy.deepcopy(agent_a)
  # a_backup.hist =[]
  n_buffers = epochs * int(a_backup.memory.obs_now.shape[0]/a_backup.batch_size)
  print(n_buffers, 'in cycle n_buffers')
  print(a_backup.memory.obs_now.shape[0], 'in cycle memory size')
  scheduler_1 = T.optim.lr_scheduler.OneCycleLR(a_backup.q_eval_1.optimizer, max_lr = max_lr, epochs=1, steps_per_epoch = n_buffers)
  lrs=[]
  for j in range(n_buffers):
    a_backup.learn()
    lrs.append(agent_a.q_eval_1.optimizer.param_groups[0]['lr'])
    scheduler_1.step()
  return a_backup