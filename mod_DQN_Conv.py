# -*- coding: utf-8 -*-
"""Untitled48.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QvmyUtRjVoxuDhjUxKG_XAesHiDeTtKR
"""

import numpy as np
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch as T
from mod_agents import *
from mod_memory import *

class ConvQNetwork_MSE(nn.Module):
    def __init__(self, lr,  n_actions, input_dims, lr_d=1e-4, lr_a=1e-5, batch_size_decide=4096, batch_size=64):
        super(ConvQNetwork_MSE, self).__init__()
###
        self.hand_cfc = nn.Linear(36, 1)
        self.hand_c1 = nn.Conv1d(in_channels = 6, out_channels = 6, kernel_size=1)
        self.hand_c3 = nn.Conv1d(in_channels = 6 , out_channels= 6, kernel_size=3 , padding=1)
        self.hand_c5 = nn.Conv1d(in_channels = 6, out_channels= 6, kernel_size = 5 , padding =2)
        self.hand_ch = nn.Conv1d(in_channels=18, out_channels=24, kernel_size=3, stride = 2)
        self.hand2_ch = nn.Conv1d(in_channels=24, out_channels=36, kernel_size=3, stride = 2)
        self.fc1_hand = nn.Linear(72, 256)
        self.fc2_mhand = nn.Linear(256,10)
        self.fc2_ohand  = nn.Linear(256,10)
###
        self.conv_fc = nn.Linear(36, 1)
        self.conv_h_1 = nn.Conv1d(in_channels = 7, out_channels = 7, kernel_size=1)
        self.conv_h_3 = nn.Conv1d(in_channels = 7 , out_channels= 7, kernel_size=3 , padding=1)
        self.conv_h_5 = nn.Conv1d(in_channels = 7, out_channels= 7, kernel_size = 5 , padding =2)
        self.conv2_h = nn.Conv1d(in_channels=21, out_channels=24, kernel_size=3, stride = 2)
        self.conv3_h = nn.Conv1d(in_channels=24, out_channels=36, kernel_size=3, stride = 2)

        self.conv_a_1 = nn.Conv1d(in_channels = 9, out_channels = 4, kernel_size = 1)
        self.conv_a_3 = nn.Conv1d(in_channels = 9 , out_channels = 4, kernel_size = 3 , padding = 1)
        self.conv_a_5 = nn.Conv1d(in_channels = 9, out_channels = 4, kernel_size = 5 , padding = 2)
        self.conv2_a = nn.Conv1d(in_channels=12, out_channels=24, kernel_size=2, stride = 2)
        self.conv3_a = nn.Conv1d(in_channels=24, out_channels=48, kernel_size=2, stride = 2)

        self.conv_s_1 = nn.Conv1d(in_channels = 5, out_channels = 5, kernel_size = 1)
        self.conv_s_3 = nn.Conv1d(in_channels = 5 , out_channels = 5, kernel_size = 3 , padding = 1)
        self.conv2_s = nn.Conv1d(in_channels=10, out_channels=20, kernel_size=2, stride = 2)

        self.value = nn.Linear(256, 1)

        self.fc1_abs = nn.Linear(84, 256)
        self.fc2_abs = nn.Linear(256,n_actions)

        self.optimizer = optim.RMSprop(self.parameters(), lr=lr, momentum = 0.9)
        self.optimizer_decide = optim.RMSprop(self.parameters(), lr=lr_d, momentum = 0.9)
        self.criterion = nn.SmoothL1Loss(reduction='none')
        self.criterion_decide = nn.BCEWithLogitsLoss()
        
        self.optimizer_hand = optim.RMSprop(self.parameters(), lr=lr, momentum = 0.9)      
        self.hand_loss1 = nn.CrossEntropyLoss()
        self.hand_loss2 = nn.CrossEntropyLoss()

        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')
        self.to(self.device)
        
        self.cards_b = T.zeros((batch_size,10)).to(self.device)
        self.flop1_b = T.zeros((batch_size,10)).to(self.device)
        self.flop2_b = T.zeros((batch_size,10)).to(self.device)
        self.flop3_b = T.zeros((batch_size,10)).to(self.device)

        self.cards_b_d = T.zeros((batch_size_decide,10)).to(self.device)
        self.flop1_b_d = T.zeros((batch_size_decide,10)).to(self.device)
        self.flop2_b_d = T.zeros((batch_size_decide,10)).to(self.device)
        self.flop3_b_d = T.zeros((batch_size_decide,10)).to(self.device)
        
        self.cards_i = T.zeros((1,10)).to(self.device)
        self.flop1_i = T.zeros((1,10)).to(self.device)        
        self.flop2_i = T.zeros((1,10)).to(self.device)
        self.flop3_i = T.zeros((1,10)).to(self.device)
        
        self.dummy_b = T.zeros((batch_size,10)).to(self.device)
        self.dummy_i = T.zeros((1,10)).to(self.device)
##################################################################################
    def forward_SA(self, state):
        d = state[:,1].view(-1,1)
        d_a = T.cat((d,d,d,d,d,d),1)
        act = T.stack((state[:,13:19],state[:,19:25],state[:,25:31],state[:,31:37],state[:,37:43],state[:,43:49],state[:,49:55],state[:,55:61],d_a),1)
        conv_a_1_o = F.relu(self.conv_a_1(act))
        conv_a_3_o = F.relu(self.conv_a_3(act))
        conv_a_5_o = F.relu(self.conv_a_5(act))
        a_o = T.cat([conv_a_1_o, conv_a_3_o,conv_a_5_o],1)
        a_o2 = F.relu(self.conv2_a(a_o))
        a_o3 = F.relu(self.conv3_a(a_o2))
        AA = a_o3.view(-1, 48*1)
        
        d_s = T.cat((d,d),1)
        put = T.cat((state[:,8].view(-1,1),state[:,8].view(-1,1)),1)
        stack = T.stack((state[:,2:4],state[:,4:6],state[:,6:8],put,d_s),1)
        conv_s_1_o = F.relu(self.conv_s_1(stack))
        conv_s_3_o = F.relu(self.conv_s_3(stack))
        s_o = T.cat([conv_s_1_o, conv_s_3_o],1)
        s_o2 = F.relu(self.conv2_s(s_o))
        S = s_o2.view(-1, 20*1)
        return S, AA
##########
    def forward(self, state, masks, use='state'):
        
        if state.shape[0] >1:
            cards = self.cards_b.zero_()
            flop1 = self.flop1_b.zero_()
            flop2 = self.flop2_b.zero_()
            flop3 = self.flop3_b.zero_()
            dummy = self.dummy_b.zero_()
        else:
            cards = self.cards_i.zero_()
            flop1 = self.flop1_i.zero_()
            flop2 = self.flop2_i.zero_() 
            flop3 = self.flop3_i.zero_() 
            dummy = self.dummy_i.zero_()

        if use== 'state':
            s = state[:,0].view(-1,1).long()
            cards.scatter_(1,s,1)
        else:
            cards = T.softmax(use, dim=-1)
    
        f1 = state[:,10].view(-1,1).long()
        flop1.scatter_(1,f1,1)
        f2 = state[:,11].view(-1,1).long()
        flop2.scatter_(1,f2,1)        
        f3 = state[:,12].view(-1,1).long()
        flop3.scatter_(1,f3,1)
        
        sf1 = cards + flop1 
        sf2 = cards + flop1 + flop2
        sf3 = cards + flop1 + flop2 + flop3
        hand = T.stack((cards,flop1,sf1,flop2,sf2,flop3,sf3),1)
        conv_h_1_o = F.relu(self.conv_h_1(hand))
        conv_h_3_o = F.relu(self.conv_h_3(hand))
        conv_h_5_o = F.relu(self.conv_h_5(hand))
        h_o = T.cat([conv_h_1_o, conv_h_3_o, conv_h_5_o],1)
        h_o2 = F.relu(self.conv2_h(h_o))
        h_o3 = F.relu(self.conv3_h(h_o2))
        H = h_o3.view(-1, 36*1)
        conv_done_1 = self.conv_fc(H)
##########
        hand_0 = T.stack((cards,dummy,cards,dummy,cards,dummy,cards),1)
        conv_h_1_o_0 = F.relu(self.conv_h_1(hand_0))
        conv_h_3_o_0 = F.relu(self.conv_h_3(hand_0))
        conv_h_5_o_0 = F.relu(self.conv_h_5(hand_0))
        h_o_0 = T.cat([conv_h_1_o_0, conv_h_3_o_0, conv_h_5_o_0],1)
        h_o2_0 = F.relu(self.conv2_h(h_o_0))
        h_o3_0 = F.relu(self.conv3_h(h_o2_0))
        H_0 = h_o3_0.view(-1, 36*1)
        conv_done_1_0 = self.conv_fc(H_0)
        ###
        hand_1 = T.stack((cards,flop1,sf1,dummy,sf1,dummy,sf1),1)
        conv_h_1_o_1 = F.relu(self.conv_h_1(hand_1))
        conv_h_3_o_1 = F.relu(self.conv_h_3(hand_1))
        conv_h_5_o_1 = F.relu(self.conv_h_5(hand_1))
        h_o_1 = T.cat([conv_h_1_o_1, conv_h_3_o_1, conv_h_5_o_1],1)
        h_o2_1 = F.relu(self.conv2_h(h_o_1))
        h_o3_1 = F.relu(self.conv3_h(h_o2_1))
        H_1 = h_o3_1.view(-1, 36*1)
        conv_done_1_1 = self.conv_fc(H_1)
        ###
        hand_2 = T.stack((cards,flop1,sf1,flop2,sf2,dummy,sf2),1)
        conv_h_1_o_2 = F.relu(self.conv_h_1(hand_2))
        conv_h_3_o_2 = F.relu(self.conv_h_3(hand_2))
        conv_h_5_o_2 = F.relu(self.conv_h_5(hand_2))
        h_o_2 = T.cat([conv_h_1_o_2, conv_h_3_o_2, conv_h_5_o_2],1)
        h_o2_2 = F.relu(self.conv2_h(h_o_2))
        h_o3_2 = F.relu(self.conv3_h(h_o2_2))
        H_2 = h_o3_1.view(-1, 36*1)
        conv_done_1_2 = self.conv_fc(H_2)
        
        return conv_done_1_0, conv_done_1_1, conv_done_1_2, conv_done_1
        
####################################################################################        
    def forward_hands(self, state, masks):
        if state.shape[0] >1:
            flop1 = self.flop1_b.zero_()
            flop2 = self.flop2_b.zero_()
            flop3 = self.flop3_b.zero_()
            dummy = self.dummy_b.zero_()
        else:
            flop1 = self.flop1_i.zero_()
            flop2 = self.flop2_i.zero_() 
            flop3 = self.flop3_i.zero_() 
            dummy = self.dummy_i.zero_()

        f1 = state[:,10].view(-1,1).long()
        flop1.scatter_(1,f1,1)
        f2 = state[:,11].view(-1,1).long()
        flop2.scatter_(1,f2,1)        
        f3 = state[:,12].view(-1,1).long()
        flop3.scatter_(1,f3,1)
        
        sf1 = flop1 
        sf2 = flop1 + flop2
        sf3 = flop1 + flop2 + flop3

        hand = T.stack((flop1,sf1,flop2,sf2,flop3,sf3),1)
        
        conv_h_1_o = F.relu(self.hand_c1(hand))
        conv_h_3_o = F.relu(self.hand_c3(hand))
        conv_h_5_o = F.relu(self.hand_c5(hand))
        h_o = T.cat([conv_h_1_o, conv_h_3_o, conv_h_5_o],1)
        h_o2 = F.relu(self.hand_ch(h_o))
        h_o3 = F.relu(self.hand2_ch(h_o2))
        H = h_o3.view(-1, 36*1)
        conv_done_1 = self.hand_cfc(H)
##########
        hand_0 = T.stack((dummy,dummy,dummy,dummy,dummy,dummy),1)
        conv_h_1_o_0 = F.relu(self.hand_c1(hand_0))
        conv_h_3_o_0 = F.relu(self.hand_c3(hand_0))
        conv_h_5_o_0 = F.relu(self.hand_c5(hand_0))
        h_o_0 = T.cat([conv_h_1_o_0, conv_h_3_o_0, conv_h_5_o_0],1)
        h_o2_0 = F.relu(self.hand_ch(h_o_0))
        h_o3_0 = F.relu(self.hand2_ch(h_o2_0))
        H_0 = h_o3_0.view(-1, 36*1)
        conv_done_1_0 = self.hand_cfc(H_0)
        ###
        hand_1 = T.stack((flop1,sf1,dummy,sf1,dummy,sf1),1)
        conv_h_1_o_1 = F.relu(self.hand_c1(hand_1))
        conv_h_3_o_1 = F.relu(self.hand_c3(hand_1))
        conv_h_5_o_1 = F.relu(self.hand_c5(hand_1))
        h_o_1 = T.cat([conv_h_1_o_1, conv_h_3_o_1, conv_h_5_o_1],1)
        h_o2_1 = F.relu(self.hand_ch(h_o_1))
        h_o3_1 = F.relu(self.hand2_ch(h_o2_1))
        H_1 = h_o3_1.view(-1, 36*1)
        conv_done_1_1 = self.hand_cfc(H_1)
        ###
        hand_2 = T.stack((flop1,sf1,flop2,sf2,dummy,sf2),1)
        conv_h_1_o_2 = F.relu(self.hand_c1(hand_2))
        conv_h_3_o_2 = F.relu(self.hand_c3(hand_2))
        conv_h_5_o_2 = F.relu(self.hand_c5(hand_2))
        h_o_2 = T.cat([conv_h_1_o_2, conv_h_3_o_2, conv_h_5_o_2],1)
        h_o2_2 = F.relu(self.hand_ch(h_o_2))
        h_o3_2 = F.relu(self.hand2_ch(h_o2_2))
        H_2 = h_o3_1.view(-1, 36*1)
        conv_done_1_2 = self.hand_cfc(H_2)
        
        S, AA = self.forward_SA(state)
        
        conv_done = T.cat([conv_done_1_0,conv_done_1_1,conv_done_1_2,conv_done_1, S.detach(), AA.detach()],1)
        fc1_abs_o = F.relu(self.fc1_hand(conv_done))
        mhand = self.fc2_mhand(fc1_abs_o)
        ohand = self.fc2_ohand(fc1_abs_o)
        
        return mhand, ohand, (conv_done_1_0, conv_done_1_1, conv_done_1_2, conv_done_1)
####################################################################
    def forward_all(self, state, masks):
        conv_done_1_0, conv_done_1_1, conv_done_1_2, conv_done_1 = self.forward(state, masks)
##########
        S, AA = self.forward_SA(state)
##########
        mhand, ohand, (chand0, chand1, chand2, chand3) = self.forward_hands(state, masks)
##########
        mhand = mhand.detach()
        ohand = ohand.detach()
        m0, m1, m2, m3 = self.forward(state, masks, use = mhand)
        o0, o1, o2, o3 = self.forward(state, masks, use = ohand)

        conv_done = T.cat([conv_done_1_0,conv_done_1_1,conv_done_1_2,conv_done_1, chand0.detach(), chand1.detach(), chand2.detach(), chand3.detach(), m0, m1, m2, m3, o0, o1, o2, o3, S, AA],1)
        fc1_abs_o = F.relu(self.fc1_abs(conv_done))
        fc2_abs_o = self.fc2_abs(fc1_abs_o)
        fc2_abs_o = fc2_abs_o * masks
        fc2_abs_o = fc2_abs_o.clamp(-25,25)
        value_o = self.value(fc1_abs_o)

        return fc2_abs_o, value_o
####################################################################
    def forward_decide(self, state):
        if state.shape[0] >1:
            cards = self.cards_b_d.zero_()
            flop1 = self.flop1_b_d.zero_()
            flop2 = self.flop2_b_d.zero_()
            flop3 = self.flop3_b_d.zero_()
        else:
            cards = self.cards_i.zero_()
            flop1 = self.flop1_i.zero_()
            flop2 = self.flop2_i.zero_() 
            flop3 = self.flop3_i.zero_() 
        s = state[:,0].view(-1,1).long()
        cards.scatter_(1,s,1)
        f1 = state[:,10].view(-1,1).long()
        flop1.scatter_(1,f1,1)
        f2 = state[:,11].view(-1,1).long()
        flop2.scatter_(1,f2,1)        
        f3 = state[:,12].view(-1,1).long()
        flop3.scatter_(1,f3,1)
        sf1 = cards + flop1 
        sf2 = cards + flop1 + flop2
        sf3 = cards + flop1 + flop2 + flop3
        hand = T.stack((cards,flop1,sf1,flop2,sf2,flop3,sf3),1)
        conv_h_1_o = F.relu(self.conv_h_1(hand))
        conv_h_3_o = F.relu(self.conv_h_3(hand))
        conv_h_5_o = F.relu(self.conv_h_5(hand))
        h_o = T.cat([conv_h_1_o, conv_h_3_o, conv_h_5_o],1)
        h_o2 = F.relu(self.conv2_h(h_o))
        h_o3 = F.relu(self.conv3_h(h_o2))
        H = h_o3.view(-1, 36*1)
        conv_done_1 = self.conv_fc(H)
        return conv_done_1


import os
class CONV_DDQN_MSE():

  def __init__(self, gamma=1, epsilon=1, lr=1e-4, lr_d = 1e-4, lr_a=1e-5, n_actions=7, input_dims=[62,], temp = 1,
                mem_size=1000, batch_size=64, batch_size_decide=4096, eps_min=0.010, eps_dec=0, mem_size_decide = 10000,
                replace=2, algo='DQN', env_name='Poker4', chkpt_dir='/content'):

    self.name = 'CONV_DDQN_MSE'
    self.hist=[]
    self.hist_decide = []
    self.hist_AV = []
    self.hist_hand1=[]
    self.hist_hand2=[]
    self.hist_hands = []
    self.force=1
    self.random = Agent_Prob_Equal()
    self.mem_size = mem_size
    self.gamma = gamma
    self.epsilon = epsilon
    self.lr = lr
    self.lr_d = lr_d
    self.lr_a = lr_a
    self.n_actions = n_actions
    self.input_dims = input_dims
    self.batch_size = batch_size
    self.batch_size_decide = batch_size_decide
    self.eps_min = eps_min
    self.eps_dec = eps_dec
    self.algo = algo
    self.env_name = env_name
    self.chkpt_dir = chkpt_dir
    self.action_space = [i for i in range(n_actions)]
    self.learn_step_counter = 0
    self.temp = 1
    self.efactor = 0.01
    self.c2 = .01
    self.c1 = 1

    self.mem_size_decide = mem_size_decide

    self.memory = MCT_Memory(num_samples = self.mem_size, input_shape=self.input_dims[0])

    self.decide_memory = MCT_decide(num_rounds=4, starting_ronds= [1,2,3,4],num_samples=self.mem_size_decide)


    self.q_eval_1 = ConvQNetwork_MSE(self.lr, self.n_actions,
                                lr_d = self.lr_d, input_dims=self.input_dims, batch_size=self.batch_size, batch_size_decide= self.batch_size_decide)
                                
    self.q_eval_AV = ConvQNetwork_MSE(self.lr, self.n_actions,
                                lr_d = self.lr_d, input_dims=self.input_dims, batch_size=self.batch_size, batch_size_decide= self.batch_size_decide)

  def probs(self, opts, obs):
    # print('using fucking probs?!')
    if np.random.random() > self.epsilon:
        state = T.tensor(obs, dtype=T.float).to(self.q_eval_1.device)
        options = T.tensor(opts, dtype=T.long).to(self.q_eval_1.device)
        advantage, _= self.q_eval_1.forward_all(state, options)
        advantage = advantage + (options + 1e-45).log()
        advantage = T.nn.functional.softmax(advantage/self.temp, dim=-1)         
        advantage = advantage * options
        probs = advantage.detach().cpu().numpy()[0]
        probs=probs.tolist()
    else:
        probs = self.random.probs(opts, obs)
    if np.sum(probs) != 1:
      probs[np.argmax(probs)]+= 1-np.sum(probs)
    if probs[0] < 0:
      probs[0]=0  
    return probs

  def action(self, opts, obs):
      probs = self.probs(opts, obs)
      index= np.random.choice(np.arange(7), p=probs)
      action = [0,0,0,0,0,0,0]
      action[index]=1
      if -1 in opts - np.array(action):
        print('opts', opts)
        print('probs', probs)
        print("illegal action selected", action )
      return action

  def sample_memory(self):
      state, reward, mask, act = self.memory.cont_buffer(self.batch_size)
      states = T.tensor(state, dtype=T.float).to(self.q_eval_1.device)
      rewards = T.tensor(reward, dtype=T.float).to(self.q_eval_1.device)
      masks = T.tensor(mask, dtype=T.long).to(self.q_eval_1.device)
      acts = T.tensor(act, dtype=T.long).to(self.q_eval_1.device)
      acts = np.nonzero(acts)[:,1]
      
      tm = states[:,0].view(-1,1).long()
      to = states[:,-1].view(-1,1).long()      

      return states, rewards, masks, acts, tm, to


  def sample_decide(self):
      state, reward = self.decide_memory.cont_buffer(self.batch_size_decide)
      states = T.tensor(state, dtype=T.float).to(self.q_eval_1.device)
      rewards = T.tensor(reward, dtype=T.float).to(self.q_eval_1.device)

      return states, rewards

###############
  def learn_AC_ppo_V(self, agent_b, debug = False):
    print('using Vs')
    self.q_eval_AV.optimizer_decide.zero_grad()
    self.q_eval_AV.optimizer.zero_grad()
    self.q_eval_1.optimizer_hand.zero_grad()
    self.q_eval_1.optimizer_decide.zero_grad()
    self.q_eval_1.optimizer.zero_grad()
    states, rewards, masks, acts, _, _= self.sample_memory()
    
    indices = np.arange(self.batch_size)
    self.q_eval_AV.optimizer_decide.zero_grad()
    self.q_eval_AV.optimizer.zero_grad()
    self.q_eval_1.optimizer_hand.zero_grad()
    self.q_eval_1.optimizer_decide.zero_grad()
    self.q_eval_1.optimizer.zero_grad()
    rewards = T.sum(rewards, axis=1).view(1,-1)

    _, V= self.q_eval_AV.forward_all(states,masks)
    V = V.view(1,-1)
    if debug: print(V, 'V')
    q_target_V = rewards.view(1,-1)
    loss_array_V = self.q_eval_1.criterion(V, q_target_V)
    loss_V = T.mean(loss_array_V)
    loss_V.backward()
    self.q_eval_AV.optimizer.step()
    self.hist_AV.append(loss_V.data.item())
    
    q_pred, _ = self.q_eval_1.forward_all(states,masks)
    q_pred_old, _ = agent_b.q_eval_1.forward_all(states,masks)
    
    q_pred = q_pred + (masks+1e-45).log()
    prob_now = T.softmax(q_pred, dim=-1)
    
    q_pred, _ = self.q_eval_1.forward_all(states,masks)
    q_pred = q_pred + (masks+1e-45).log()
    prob_now = T.softmax(q_pred, dim=-1)[indices, acts]
    
    q_pred_old, _ = agent_b.q_eval_1.forward_all(states,masks)
    q_pred_old = q_pred_old + (masks+1e-45).log()
    prob_old = T.softmax(q_pred_old, dim=-1)[indices, acts]

    target = rewards - V.detach().view(1,-1)
    if debug: print(target, 'target')    
    r = (prob_now +1e-20)/ (prob_old+1e-20) * target
    rc = T.clamp(r, 1-self.efactor, 1+self.efactor) * target
    rA = T.stack([r, rc])
    rmin = T.min(rA, axis=0)[0]
    if debug: print(target, 'rmin') 
    loss_array_ppo = - rmin
    
    log_probs = T.log_softmax(q_pred,dim=-1) * masks 
    entropy = log_probs[indices, acts] * prob_now  
    if debug: print(entropy, 'entropy') 
    loss_array = loss_array_ppo.view(1,-1) + self.c2 * entropy.view(1,-1)
    
    loss = T.mean(loss_array)
    loss.backward()
    self.q_eval_1.optimizer.step()
    self.hist.append(loss.data.item())
    
###############
  def learn_hands(self, debug = False):
    states, rewards, masks, acts, tm, to= self.sample_memory()
    self.q_eval_AV.optimizer_decide.zero_grad()
    self.q_eval_AV.optimizer.zero_grad()
    self.q_eval_1.optimizer_hand.zero_grad()
    self.q_eval_1.optimizer_decide.zero_grad()
    self.q_eval_1.optimizer.zero_grad()
    print('learn hands')
    mhand, ohand, _= self.q_eval_1.forward_hands(states,masks)
    tm = (tm).flatten()
    if debug: print(tm, 'tm')
    if debug: print(mhand, 'mhand')
    loss1 = self.q_eval_1.hand_loss1(mhand, tm)
    # loss1.backward()
    # self.q_eval_1.optimizer_hand.step()
    # self.hist_hand1.append(loss1.data.item())
    # self.q_eval_AV.optimizer_decide.zero_grad()
    # self.q_eval_AV.optimizer.zero_grad()
    # self.q_eval_1.optimizer_hand.zero_grad()
    # self.q_eval_1.optimizer_decide.zero_grad()
    # self.q_eval_1.optimizer.zero_grad()
    to = (to).flatten()
    if debug: print(to, 'to')
    if debug: print(ohand, 'ohand')
    loss2 = self.q_eval_1.hand_loss2(ohand, to)
    
    loss_hands = loss1 + loss2
    loss_hands.backward()
    self.q_eval_1.optimizer_hand.step()
    
    self.hist_hand1.append(loss1)
    self.hist_hand2.append(loss2)  
    
    self.hist_hands.append(loss_hands.data.item())
#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
  def learn_all(self, agent_b, debug = False):
    print('using Vs')
    self.q_eval_AV.optimizer_decide.zero_grad()
    self.q_eval_AV.optimizer.zero_grad()
    self.q_eval_1.optimizer_hand.zero_grad()
    self.q_eval_1.optimizer_decide.zero_grad()
    self.q_eval_1.optimizer.zero_grad()
    states, rewards, masks, acts, tm, to= self.sample_memory()
    
    indices = np.arange(self.batch_size)
    self.q_eval_AV.optimizer_decide.zero_grad()
    self.q_eval_AV.optimizer.zero_grad()
    self.q_eval_1.optimizer_hand.zero_grad()
    self.q_eval_1.optimizer_decide.zero_grad()
    self.q_eval_1.optimizer.zero_grad()
    rewards = T.sum(rewards, axis=1).view(1,-1)

    _, V = self.q_eval_AV.forward_all(states,masks)
    V = V.view(1,-1)
    if debug: print(V, 'V')
    q_target_V = rewards.view(1,-1)
    loss_array_V = self.q_eval_1.criterion(V, q_target_V)
    loss_V = T.mean(loss_array_V)
    loss_V.backward()
    self.q_eval_AV.optimizer.step()
    self.hist_AV.append(loss_V.data.item())
    
    q_pred, _ = self.q_eval_1.forward_all(states,masks)
    q_pred_old, _ = agent_b.q_eval_1.forward_all(states,masks)
    
    q_pred = q_pred + (masks+1e-45).log()
    prob_now = T.softmax(q_pred, dim=-1)
    
    q_pred, _ = self.q_eval_1.forward_all(states,masks)
    q_pred = q_pred + (masks+1e-45).log()
    prob_now = T.softmax(q_pred, dim=-1)[indices, acts]
    
    q_pred_old, _ = agent_b.q_eval_1.forward_all(states,masks)
    q_pred_old = q_pred_old + (masks+1e-45).log()
    prob_old = T.softmax(q_pred_old, dim=-1)[indices, acts]

    target = rewards - V.detach().view(1,-1)
    if debug: print(target, 'target')    
    r = (prob_now +1e-20)/ (prob_old+1e-20) * target
    rc = T.clamp(r, 1-self.efactor, 1+self.efactor) * target
    rA = T.stack([r, rc])
    rmin = T.min(rA, axis=0)[0]
    if debug: print(target, 'rmin') 
    loss_array_ppo = - rmin
    
    log_probs = T.log_softmax(q_pred,dim=-1) * masks 
    entropy = log_probs[indices, acts] * prob_now  
    if debug: print(entropy, 'entropy') 
    loss_array = loss_array_ppo.view(1,-1) + self.c2 * entropy.view(1,-1)
    
    loss = T.mean(loss_array)
    loss.backward()
    self.q_eval_1.optimizer.step()
    self.hist.append(loss.data.item())
#
    self.q_eval_AV.optimizer_decide.zero_grad()
    self.q_eval_AV.optimizer.zero_grad()
    self.q_eval_1.optimizer_hand.zero_grad()
    self.q_eval_1.optimizer_decide.zero_grad()
    self.q_eval_1.optimizer.zero_grad()
    print('learn hands')
    mhand, ohand, _ = self.q_eval_1.forward_hands(states,masks)
    tm = (tm).flatten()
    if debug: print(tm, 'tm')
    if debug: print(mhand, 'mhand')
    loss1 = self.q_eval_1.hand_loss1(mhand, tm)
    # loss1.backward()
    # self.q_eval_1.optimizer_hand.step()
    # self.hist_hand1.append(loss1.data.item())
    # self.q_eval_AV.optimizer_decide.zero_grad()
    # self.q_eval_AV.optimizer.zero_grad()
    # self.q_eval_1.optimizer_hand.zero_grad()
    # self.q_eval_1.optimizer_decide.zero_grad()
    # self.q_eval_1.optimizer.zero_grad()
    to = (to).flatten()
    if debug: print(to, 'to')
    if debug: print(ohand, 'ohand')
    loss2 = self.q_eval_1.hand_loss2(ohand, to)
    
    loss_hands = loss1 + loss2
    loss_hands.backward()
    self.q_eval_1.optimizer_hand.step()
    
    self.hist_hand1.append(loss1)
    self.hist_hand2.append(loss2)  
    
    self.hist_hands.append(loss_hands.data.item())
    
##############
  def learn_decide(self):
    self.q_eval_1.optimizer_decide.zero_grad()
    self.q_eval_1.optimizer.zero_grad()
    states, rewards = self.sample_decide()
    q_pred = self.q_eval_1.forward_decide(states)
    q_target= rewards

    loss_decide = self.q_eval_1.criterion_decide(q_pred, q_target)
    loss_decide.backward()
    self.q_eval_1.optimizer_decide.step()
    self.hist_decide.append(loss_decide.data.item())